{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35c10ff-6d96-44e4-b9f6-b66e2b69e8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n",
      "\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "\u001b[33mWARNING: Skipping deepface as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping deep-sort-realtime as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: tensorflow 2.17.0\n",
      "Uninstalling tensorflow-2.17.0:\n",
      "  Successfully uninstalled tensorflow-2.17.0\n",
      "Found existing installation: torch 2.4.1.post100\n",
      "Uninstalling torch-2.4.1.post100:\n",
      "  Successfully uninstalled torch-2.4.1.post100\n",
      "Found existing installation: torchvision 0.19.1\n",
      "Uninstalling torchvision-0.19.1:\n",
      "  Successfully uninstalled torchvision-0.19.1\n",
      "\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: scikit-learn 1.5.2\n",
      "Uninstalling scikit-learn-1.5.2:\n",
      "  Successfully uninstalled scikit-learn-1.5.2\n",
      "\u001b[33mWARNING: Skipping pinecone as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y opencv-python opencv-python-headless numpy deepface deep-sort-realtime tensorflow torch torchvision torchaudio scikit-learn pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37103b6-1a45-4057-9ac6-ed575d5fea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting drwa.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile drwa.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(\"classrooma.mp4\")\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # Frames per second\n",
    "start_frame = int(4 * fps)  # Start at 3 seconds\n",
    "end_frame = int(8 * fps)  # End at 10 seconds\n",
    "\n",
    "embeddings_dict = {}  # Dictionary to store embeddings {track_id: embedding_list}\n",
    "\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames before the start time\n",
    "    if frame_count < start_frame:\n",
    "        continue\n",
    "\n",
    "    # Stop processing after the end time\n",
    "    if frame_count > end_frame:\n",
    "        break\n",
    "\n",
    "    # Process every 5th frame for efficiency\n",
    "    if frame_count % 5 == 0:\n",
    "        try:\n",
    "            # Resize frame for better performance\n",
    "            frame_resized = cv2.resize(frame, (640, 360))\n",
    "\n",
    "            # Detect faces\n",
    "            faces = DeepFace.extract_faces(frame_resized, detector_backend=\"retinaface\", enforce_detection=False)\n",
    "\n",
    "            detections = []\n",
    "            for face in faces:\n",
    "                facial_area = face[\"facial_area\"]\n",
    "                x, y, w, h = facial_area[\"x\"], facial_area[\"y\"], facial_area[\"w\"], facial_area[\"h\"]\n",
    "                bbox = [x, y, x + w, y + h]  # Convert to (x1, y1, x2, y2)\n",
    "\n",
    "                detections.append((bbox, 1.0))  # (bounding_box, confidence_score)\n",
    "\n",
    "            # Update tracker with detections\n",
    "            tracked_faces = tracker.update_tracks(detections, frame=frame_resized)\n",
    "\n",
    "            for track in tracked_faces:\n",
    "                if not track.is_confirmed():\n",
    "                    continue  # Ignore unconfirmed tracks\n",
    "\n",
    "                track_id = track.track_id  # Unique ID assigned by DeepSORT\n",
    "                bbox = track.to_ltrb()  # Get bounding box (x1, y1, x2, y2)\n",
    "\n",
    "                # Crop the detected face\n",
    "                x1, y1, x2, y2 = map(int, bbox)\n",
    "                face_crop = frame_resized[y1:y2, x1:x2]\n",
    "\n",
    "                # Ensure the crop is not empty\n",
    "                if face_crop.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # Get embedding for the tracked face\n",
    "                embedding = DeepFace.represent(face_crop, model_name=\"Facenet\", enforce_detection=False)\n",
    "                if embedding:\n",
    "                    embedding = embedding[0][\"embedding\"]\n",
    "                    if track_id not in embeddings_dict:\n",
    "                        embeddings_dict[track_id] = [np.array(embedding, dtype=np.float32)]\n",
    "                    else:\n",
    "                        embeddings_dict[track_id].append(np.array(embedding, dtype=np.float32))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_count}: {e}\")\n",
    "\n",
    "# Average embeddings for each tracked person\n",
    "final_embeddings = {track_id: np.mean(embeddings, axis=0) for track_id, embeddings in embeddings_dict.items()}\n",
    "\n",
    "# Convert dictionary to a NumPy array and save\n",
    "np.save(\"tracked_face_embeddings.npy\", final_embeddings)\n",
    "print(f\"Saved {len(final_embeddings)} unique face embeddings to tracked_face_embeddings.npy\")\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfa9db6-bb3f-4d53-86a1-dbde96471512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': array([-0.17045061,  0.3332695 ,  0.59908915, -0.7415862 , -0.10415559,\n",
      "        0.12151986,  0.88161117, -0.65495825, -0.16447303,  0.80027705,\n",
      "        0.2910854 ,  0.15943286,  0.13427003, -0.46339226,  0.17969814,\n",
      "        0.5544893 , -0.18968946, -1.1058036 , -0.44528726, -0.51665455,\n",
      "        0.43294275, -0.7521591 , -0.00263618,  0.29751927, -0.6230553 ,\n",
      "       -0.1891846 ,  0.24976043,  0.20649855,  0.11538564,  0.30589646,\n",
      "       -0.25506714, -0.20951882,  0.02272737, -0.5420021 ,  0.6142189 ,\n",
      "        0.04159555, -0.97948915, -0.03390833,  0.22418304,  0.2616121 ,\n",
      "       -0.26914346, -0.35589966, -0.36173147,  0.5020479 , -0.82086873,\n",
      "       -0.48208815, -0.15118298,  0.13040118, -0.78610253, -0.48140424,\n",
      "        0.00145244,  0.40780765,  0.11898398, -0.13268699,  0.11159035,\n",
      "        0.31218326,  0.01776191,  0.24724084, -0.36590043, -0.13728108,\n",
      "       -0.681348  , -0.10427935, -0.65236986, -0.2564089 ,  0.27837607,\n",
      "       -0.24725068, -0.23050654,  0.33736554,  0.26800373,  0.03482004,\n",
      "       -0.11451285,  0.04271473,  0.6215542 , -0.56115854,  0.27669752,\n",
      "        0.76572293, -0.19455446,  0.05626034, -0.06889583, -0.80646586,\n",
      "        0.3999023 , -0.0706311 , -0.57436323, -0.859165  ,  0.08632288,\n",
      "       -0.40464377, -0.09858233, -0.54572517,  0.16645776,  0.39239147,\n",
      "        0.13354467,  0.14142458,  0.5763376 , -0.04720157,  0.41470978,\n",
      "        1.2170057 , -0.47437742, -0.11641051, -0.26133052, -0.34913695,\n",
      "        0.11856814,  0.30804107, -0.50219893, -0.5192311 , -0.03903048,\n",
      "       -0.8182254 ,  0.33418083, -0.16256118,  0.76840895, -0.29493397,\n",
      "       -0.02194994,  0.6569937 ,  0.3396893 , -0.20813054, -0.3466006 ,\n",
      "        0.6183007 ,  0.31189293,  0.47417295, -0.07664683,  0.61630297,\n",
      "        0.29462758, -0.40741846,  0.42167795, -0.33090115,  0.54149675,\n",
      "        0.3314361 ,  0.5460442 , -0.10432205], dtype=float32), '2': array([-0.05078341, -0.24046426, -0.10031991, -0.65200007,  0.4127543 ,\n",
      "       -0.10080328,  0.08263458, -0.09937679, -0.03464264,  0.28733182,\n",
      "        0.49724007, -0.126315  , -0.13771684, -0.4967544 , -0.10288944,\n",
      "        0.31344184,  0.34582862, -0.59080297,  0.17160812, -0.69253737,\n",
      "       -0.04680401, -0.07805202, -0.42661196,  0.23458366, -0.13359216,\n",
      "       -0.22726278,  0.41357654,  0.33530417, -0.11387417,  0.14502841,\n",
      "        0.03324905,  0.05468247,  0.00250275, -0.33482417,  0.5415721 ,\n",
      "       -0.23363769, -0.05674632,  0.3099362 ,  0.644662  ,  0.18277143,\n",
      "        0.05829519,  0.0619456 , -0.03603542,  0.08752829, -0.83358544,\n",
      "       -0.7477248 ,  0.02463711, -0.12708783,  0.08917108,  0.17288895,\n",
      "       -0.1296784 ,  0.56479037,  0.6480416 , -0.57028997,  0.18857248,\n",
      "        0.38283864,  0.29956457,  0.19894946, -0.4087821 , -0.8277028 ,\n",
      "       -0.5330545 , -0.4568617 , -0.26081997,  0.6430865 ,  0.086777  ,\n",
      "        0.4542518 ,  0.0454695 ,  0.22113478, -0.05604949, -0.20648384,\n",
      "        0.05687682, -0.375746  ,  0.36544427, -0.71097505, -0.2667037 ,\n",
      "        0.26134664, -0.05421224,  0.5656667 , -0.41511476, -0.62346476,\n",
      "       -0.4984277 , -0.11469491,  0.0675515 ,  0.196737  ,  0.23803769,\n",
      "       -0.05153974,  0.28594813, -0.3965609 , -0.47957677,  0.6659068 ,\n",
      "        0.36244068, -0.01079078, -0.2509595 ,  0.06349543,  0.692401  ,\n",
      "        0.16166013, -0.13407694, -0.5444442 , -0.5922262 , -0.18601985,\n",
      "       -0.14790942,  0.80691046,  0.22182311, -0.4734665 , -0.01306669,\n",
      "       -0.01054107,  0.14694051, -0.07416578,  0.05323077, -0.4986801 ,\n",
      "        0.21575704,  0.42645335,  0.57653284,  0.39719272, -0.08478667,\n",
      "        0.23632625,  0.16469423,  0.477203  ,  0.20499092,  0.17958324,\n",
      "       -0.25420275, -0.15319037,  0.38094985, -0.01152759,  0.3975273 ,\n",
      "       -0.47920257,  0.26221552, -0.15865576], dtype=float32), '3': array([-0.25128537, -0.34615   , -0.1879665 , -0.48978546,  0.36920792,\n",
      "        0.11621029,  0.02760435, -0.16457869,  0.05514042,  0.10455333,\n",
      "        0.45157418, -0.11229118,  0.03320193, -0.546065  , -0.06183203,\n",
      "        0.3182003 ,  0.27570474, -0.37748706,  0.41285262, -0.5523702 ,\n",
      "       -0.05926542, -0.00236532, -0.24745663,  0.07974669, -0.00863128,\n",
      "       -0.20149371,  0.63839865,  0.23453076, -0.4097157 , -0.21161944,\n",
      "       -0.49860764,  0.17664105, -0.02256318, -0.07809903,  0.5301286 ,\n",
      "       -0.0424295 ,  0.37636432,  0.03374016,  0.4910871 , -0.07752952,\n",
      "        0.38070533,  0.14158554, -0.0514872 ,  0.02495707, -0.5666361 ,\n",
      "       -0.8014476 ,  0.16600646, -0.21803865,  0.41158235,  0.27281642,\n",
      "       -0.4639483 ,  0.18020646,  0.3915333 , -0.2905998 , -0.01663811,\n",
      "        0.26443157,  0.23092733,  0.20017722, -0.27921915, -0.76003104,\n",
      "       -0.3617339 , -0.25880176, -0.12868589,  0.6254916 , -0.02166528,\n",
      "        0.5996773 ,  0.15986055,  0.17980532,  0.10072906, -0.2804546 ,\n",
      "       -0.03879657, -0.41283518,  0.27356243, -0.47381067, -0.05237313,\n",
      "       -0.06012745,  0.00772003,  0.53664714, -0.17191929, -0.42947632,\n",
      "       -0.59265345, -0.31857914, -0.06463055,  0.20439595,  0.2823916 ,\n",
      "        0.2007884 ,  0.6353352 , -0.12500073, -0.44251093,  0.7074202 ,\n",
      "        0.3520543 ,  0.12001392, -0.11109937, -0.13781826,  0.5273447 ,\n",
      "        0.258474  , -0.18192054, -0.69013673, -0.74100536,  0.22773236,\n",
      "       -0.19470347,  0.72173756,  0.30015498, -0.5048864 ,  0.26123628,\n",
      "        0.02559074,  0.04464847, -0.24046479, -0.15285502, -0.46712002,\n",
      "        0.33843154,  0.1410362 ,  0.4608229 ,  0.6032208 , -0.25087535,\n",
      "        0.16499971, -0.12221548,  0.59179044,  0.42701498,  0.3523209 ,\n",
      "       -0.6527827 , -0.23117805,  0.46998298,  0.32416734,  0.47907177,\n",
      "       -0.35644937,  0.68524605,  0.16318686], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the saved embeddings\n",
    "embeddings = np.load(\"tracked_face_embeddings.npy\", allow_pickle=True).item()\n",
    "\n",
    "# Print the embeddings\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8daf10b-b23d-45c8-a58f-3fa1f002f914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting newdrwa.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile newdrwa.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pinecone import Pinecone\n",
    "from deepface import DeepFace\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"pcsk_6m9iGK_SJCAHA6WqusNpxon1s3M9W6VAjfZk86H7mQrrGwQPA67G2nVkhkKLCWcDEas6Kv\")\n",
    "index = pc.Index(\"deepface\")  # Ensure this index uses Euclidean distance\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(\"classrooma.mp4\")\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "start_frame = int(4 * fps)\n",
    "end_frame = int(8 * fps)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "# Dictionary to store embeddings for each tracked face {track_id: [embedding1, embedding2, ...]}\n",
    "embeddings_dict = {}\n",
    "\n",
    "def query_pinecone(embedding, index, threshold=0.8):\n",
    "    \"\"\"Query Pinecone for similar faces using Euclidean distance.\"\"\"\n",
    "    # Normalize the embedding\n",
    "    embedding_normalized = embedding / np.linalg.norm(embedding)\n",
    "    embedding_list = embedding_normalized.tolist()\n",
    "\n",
    "    response = index.query(\n",
    "        namespace=\"default\",\n",
    "        vector=embedding_list,\n",
    "        top_k=1,  # Get the most similar face\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    print(f\"Pinecone Response: {response}\")  # Debugging: Print the full response\n",
    "\n",
    "    if response.get(\"matches\"):\n",
    "        best_match = response[\"matches\"][0]\n",
    "        print(f\"Best Match Score: {best_match['score']}\")  # Debugging: Print the score\n",
    "        if best_match[\"score\"] <= threshold:  # Euclidean distance: lower is better\n",
    "            return best_match.get(\"metadata\", {}).get(\"name\", \"Unknown\"), best_match[\"score\"]\n",
    "    return None, 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames before start time\n",
    "    if frame_count < start_frame:\n",
    "        continue\n",
    "    # Stop processing after the end time\n",
    "    if frame_count > end_frame:\n",
    "        break\n",
    "\n",
    "    # Process every 5th frame for efficiency\n",
    "    if frame_count % 5 == 0:\n",
    "        try:\n",
    "            frame_resized = cv2.resize(frame, (640, 360))\n",
    "\n",
    "            # Detect faces\n",
    "            faces = DeepFace.extract_faces(frame_resized, detector_backend=\"retinaface\", enforce_detection=False)\n",
    "\n",
    "            detections = []\n",
    "            for face in faces:\n",
    "                facial_area = face[\"facial_area\"]\n",
    "                x, y, w, h = facial_area[\"x\"], facial_area[\"y\"], facial_area[\"h\"], facial_area[\"w\"]\n",
    "                bbox = [x, y, x + w, y + h]  # Convert to (x1, y1, x2, y2)\n",
    "                detections.append((bbox, 1.0))  # (bounding_box, confidence_score)\n",
    "\n",
    "            # Update tracker\n",
    "            tracked_faces = tracker.update_tracks(detections, frame=frame_resized)\n",
    "\n",
    "            for track in tracked_faces:\n",
    "                if not track.is_confirmed():\n",
    "                    continue  # Ignore unconfirmed tracks\n",
    "\n",
    "                track_id = str(track.track_id)  # Convert track ID to string\n",
    "                bbox = track.to_ltrb()  # Bounding box (x1, y1, x2, y2)\n",
    "                x1, y1, x2, y2 = map(int, bbox)\n",
    "                face_crop = frame_resized[y1:y2, x1:x2]\n",
    "\n",
    "                if face_crop.size == 0 or face_crop.shape[0] < 10 or face_crop.shape[1] < 10:\n",
    "                    continue  # Skip if no face detected or face is too small\n",
    "\n",
    "                # Generate face embedding\n",
    "                embedding = DeepFace.represent(face_crop, model_name=\"Facenet\", enforce_detection=False)\n",
    "                if embedding:\n",
    "                    embedding = embedding[0][\"embedding\"]\n",
    "\n",
    "                    # Store embedding in the dictionary\n",
    "                    if track_id not in embeddings_dict:\n",
    "                        embeddings_dict[track_id] = [np.array(embedding, dtype=np.float32)]\n",
    "                    else:\n",
    "                        embeddings_dict[track_id].append(np.array(embedding, dtype=np.float32))\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing frame {frame_count}: {str(e)}\")\n",
    "\n",
    "# Average embeddings for each tracked face\n",
    "final_embeddings = {track_id: np.mean(embeddings, axis=0) for track_id, embeddings in embeddings_dict.items()}\n",
    "\n",
    "# Query Pinecone with averaged embeddings\n",
    "for track_id, avg_embedding in final_embeddings.items():\n",
    "    print(f\"Averaged Embedding for Track ID {track_id}: {avg_embedding}\")  # Debugging: Print the averaged embedding\n",
    "    person_name, distance_score = query_pinecone(avg_embedding, index, threshold=0.8)\n",
    "    if person_name:\n",
    "        logging.info(f\"Track ID {track_id} is {distance_score:.4f} Euclidean distance from {person_name}\")\n",
    "    else:\n",
    "        logging.info(f\"No similar face found in the database for Track ID {track_id}.\")\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7bc6b7-95f3-45b8-833d-5a86b906a159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 543\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596e1238-3cfb-4db0-975c-8f48348efe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85305461-7a0e-4efd-9d41-80e80b6d6896",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepface'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepFace\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Test with a sample image\u001b[39;00m\n\u001b[1;32m      4\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeople.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepface'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a0197-9c30-4ece-8640-906ff66acc75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
